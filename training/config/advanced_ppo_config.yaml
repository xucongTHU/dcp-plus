# Advanced PPO Training Configuration for Path Planning
# This configuration is designed for better convergence and performance

# Environment parameters
state_dim: 24              # Extended state representation (x, y, heatmap, actions, etc.)
action_dim: 4              # 4-directional movement (up, right, down, left)
hidden_dim: 128            # Hidden layer size
network_layers: 3          # Number of hidden layers

# PPO hyperparameters - Tuned for better performance
learning_rate: 0.0005      # Slightly higher learning rate for faster convergence
gamma: 0.999               # Higher gamma to consider long-term rewards
lam: 0.95                  # GAE lambda
epsilon: 0.2               # PPO clip parameter
epochs: 10                 # Number of update epochs
batch_size: 128            # Larger batch size for more stable updates

# Training parameters
episodes: 15000            # Significantly more episodes for better training
max_steps: 300             # More steps to allow reaching distant goals

# Regularization
entropy_coef: 0.02         # Higher entropy to encourage exploration
dropout_rate: 0.1          # Dropout for regularization

# Advanced training techniques
use_gradient_clipping: true     # Enable gradient clipping
gradient_clip_value: 0.5        # Gradient clipping value
use_lr_scheduler: true          # Learning rate scheduling
lr_decay_rate: 0.999            # Slower learning rate decay

# Logging
log_interval: 200          # Log every N episodes

# Environment settings
env_width: 20              # Environment width
env_height: 20             # Environment height
obstacle_ratio: 0.2        # Ratio of obstacles in complex environment