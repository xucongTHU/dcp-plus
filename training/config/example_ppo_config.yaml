# Example PPO Training Configuration for Path Planning

# Environment parameters
state_dim: 2               # (x, y) position
action_dim: 4              # 4-directional movement (up, right, down, left)
hidden_dim: 128            # Hidden layer size

# PPO hyperparameters
learning_rate: 0.0003      # Learning rate
gamma: 0.99                # Discount factor
lam: 0.95                  # GAE lambda
epsilon: 0.2               # PPO clip parameter
epochs: 10                 # Number of update epochs
batch_size: 64             # Mini-batch size

# Training parameters
episodes: 5000             # Total training episodes
max_steps: 200             # Maximum steps per episode

# Exploration parameters
entropy_coef: 0.01         # Entropy coefficient for exploration

# Logging
log_interval: 100          # Log every N episodes

# Environment settings
env_width: 20              # Environment width
env_height: 20             # Environment height
obstacle_ratio: 0.2        # Ratio of obstacles in complex environment