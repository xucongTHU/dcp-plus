# PPO Training Configuration

# Environment parameters
state_dim: 2               # (x, y) position
action_dim: 4              # 4-directional movement (up, right, down, left)
hidden_dim: 64             # Hidden layer size

# PPO hyperparameters
learning_rate: 0.0003      # Learning rate
gamma: 0.99                # Discount factor
lam: 0.95                  # GAE lambda
epsilon: 0.2               # PPO clip parameter
epochs: 10                 # Number of update epochs
batch_size: 64             # Mini-batch size

# Training parameters
episodes: 10000            # Total training episodes
max_steps: 200             # Maximum steps per episode

# Exploration parameters
entropy_coef: 0.01         # Entropy coefficient for exploration

# Logging
log_interval: 100          # Log every N episodes